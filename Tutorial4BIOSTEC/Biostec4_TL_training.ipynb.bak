{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650f7c44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notebook 4 – Transfer Learning in NeuralLib\n",
    "\n",
    "In this notebook we focus on **transfer learning (TL)** with NeuLib:\n",
    "\n",
    "- Start from a pretrained ECG **ProductionModel**.\n",
    "- Build a **TransferLearningModel (TLModel)** that reuses part of the pretrained GRU encoder.\n",
    "- Add extra layers for the new task/domain.\n",
    "- Inject pretrained weights, set up freezing/unfreezing, and fine-tune on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6612d89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How exactly does NeuralLib work?\n",
    "<img src=\"tutorial_plots/NeuralLibframework.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed55ba0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 0 – Environment setup\n",
    "\n",
    "For this tutorial NeuLib is imported from a local clone of the repository.\n",
    "We temporarily add the project directory to `sys.path` so that Python can find the package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf4e7b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cell to be removed once the package is stable\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of your project directory\n",
    "project_path = os.path.abspath(\"..\")\n",
    "print(project_path)\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ffb5c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Import NeuLib modules for transfer learning\n",
    "\n",
    "We import:\n",
    "\n",
    "- The GRU-based architecture `GRUseq2seq`.\n",
    "- Dataset paths for ECG peak detection.\n",
    "- Transfer learning helpers from `model_hub`:\n",
    "  - `TLModel`, `TLFactory`\n",
    "  - `list_production_models`\n",
    "  - `build_tl_arch_config`, `build_pretrained_layer_map`, `build_freeze_phase`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818aaed1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from NeuralLib.architectures import GRUseq2seq\n",
    "from NeuralLib.config import DATASETS_ECG_G # peak detection dataset\n",
    "from NeuralLib.model_hub import (\n",
    "    TLModel, TLFactory, \n",
    "    list_production_models, \n",
    "    build_pretrained_layer_map, \n",
    "    build_tl_arch_config,\n",
    "    build_freeze_phase\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398078b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1 – Load target-domain data\n",
    "\n",
    "We first define the paths for the **target** ECG dataset:\n",
    "\n",
    "- `X`: input ECG signals.\n",
    "- `Y_BIN`: binary labels for peak detection.\n",
    "\n",
    "This dataset represents the **target domain** on which we will fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7f7f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = os.path.join(DATASETS_ECG_G, \"sample\", \"x\")\n",
    "Y_BIN = os.path.join(DATASETS_ECG_G, \"sample\", \"y_bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817c9c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2 – Load a pretrained ProductionModel\n",
    "\n",
    "NeuLib provides **ProductionModels** stored on the model hub.\n",
    "We use a `TLFactory` to:\n",
    "\n",
    "- List available pretrained models.\n",
    "- Download or load cached weights.\n",
    "- Prepare them for transfer learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08b47b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "factory = TLFactory()\n",
    "\n",
    "# List available production models\n",
    "list_production_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c485e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Select a pretrained ECGPeakDetector\n",
    "\n",
    "We now load the `ECGPeakDetector` ProductionModel.\n",
    "\n",
    "This gives us access to:\n",
    "\n",
    "- The pretrained GRU-based encoder–decoder.\n",
    "- Its architecture configuration (hidden sizes, bidirectionality, etc.).\n",
    "- The checkpoint files stored locally in the NeuLib cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014f56d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load Production Model\n",
    "factory.load_production_model(model_name=\"ECGPeakDetector\")\n",
    "prod_model = factory.models[\"ECGPeakDetector\"]\n",
    "\n",
    "print(\"\\nPretrained GRU model (encoder):\")\n",
    "print(prod_model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379c152",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 3 – Build a TransferLearningModel configuration\n",
    "\n",
    "We now decide **how much** of the pretrained model to reuse and\n",
    "**how** to extend it for the new task.\n",
    "\n",
    "Key design choices:\n",
    "\n",
    "- Number of GRU layers to reuse from the pretrained model (`reuse_n_gru_layers`).\n",
    "- Additional hidden dimensions (`extra_hid_dims`) appended on top.\n",
    "- Bidirectionality per layer (`bidir_per_layer`).\n",
    "- New task type (e.g. regression vs classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd309f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 3.1 – Define TL architecture parameters\n",
    "\n",
    "`build_tl_arch_config`:\n",
    "\n",
    "- Reads the configuration of the pretrained `prod_model`.\n",
    "- Reuses the first `reuse_n_gru_layers` layers.\n",
    "- Appends new GRU layers with `extra_hid_dims`.\n",
    "- Builds a configuration dictionary `arch_params` for the TLModel.\n",
    "- Returns `reuse_n` (the actual number of reused layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3af1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12d5447",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "arch_params, reuse_n = build_tl_arch_config(\n",
    "    prod_model=prod_model,\n",
    "    reuse_n_gru_layers=2,\n",
    "    extra_hid_dims=[128, 128],\n",
    "    bidir_per_layer=[True, True, True, False],\n",
    "    learning_rate=1e-3,\n",
    "    model_name_suffix=\"_TL_PeakDetector\",\n",
    "    task=\"regression\",\n",
    "    num_classes=1,\n",
    "    multi_label=False,    # BCEWithLogits for binary peaks\n",
    "    fc_out_bool=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9022f6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"tutorial_plots/FrozenLayer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95124f21",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tl_model = TLModel(GRUseq2seq, **arch_params)\n",
    "print(f\"TL_model structure{tl_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689abaf2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4 – Map pretrained layers and define freezing strategy\n",
    "\n",
    "To perform TL we must:\n",
    "\n",
    "1. Decide which GRU layers reuse pretrained weights.\n",
    "2. Build a **mapping** between layers in the ProductionModel and the TLModel.\n",
    "3. Choose a **freezing strategy** (which layers to freeze/unfreeze during fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b6ce0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 4.1 – Build layer mapping\n",
    "\n",
    "`build_pretrained_layer_map`:\n",
    "\n",
    "- Reads the GRU layers from the pretrained `prod_model`.\n",
    "- Matches them to the first `reuse_n` GRU layers of `tl_model`.\n",
    "- Returns a dictionary that will be used to inject weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1128b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Build layer mapping from pretrained model\n",
    "layer_mapping = build_pretrained_layer_map(prod_model, reuse_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81a572",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4.2 – Decide which layers to freeze\n",
    "\n",
    "We use `build_freeze_phase` to:\n",
    "\n",
    "- Freeze reused layers during the first phase of fine-tuning.\n",
    "- Keep newly added layers trainable.\n",
    "\n",
    "Here we use the **`new_only`** strategy:\n",
    "\n",
    "- Reused GRU layers are frozen.\n",
    "- New GRU layers remain unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61a444",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Automatic freeze layer settings\n",
    "freeze_layers, unfreeze_layers = build_freeze_phase(\n",
    "    tl_model,\n",
    "    reuse_n,\n",
    "    strategy=\"new_only\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e3a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"tutorial_plots/ExtraHiddenLayers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6a641",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5 – Inject weights and configure the TLModel\n",
    "\n",
    "We are now ready to:\n",
    "\n",
    "- Inject pretrained weights into selected GRU layers of `tl_model`.\n",
    "- Freeze and unfreeze layers according to our strategy.\n",
    "- Finalize the TLModel configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e299a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Configure a TransferLearningModel by injecting weights and managing layer freezing/unfreezing\n",
    "factory.configure_tl_model(\n",
    "    tl_model=tl_model,\n",
    "    layer_mapping=layer_mapping,\n",
    "    freeze_layers=freeze_layers,\n",
    "    unfreeze_layers=unfreeze_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adbfa2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 6 – Fine-tune the TLModel\n",
    "\n",
    "We now define training parameters for fine-tuning on the target dataset:\n",
    "\n",
    "- Number of epochs and batch size.\n",
    "- Early stopping patience.\n",
    "- Dataset name and description of the TL objective.\n",
    "- Device and logging options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee2527",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "train_params = {\n",
    "    \"path_x\": X,\n",
    "    \"path_y\": Y_BIN,\n",
    "    \"epochs\": 300,\n",
    "    \"batch_size\": 32,\n",
    "    \"patience\": 20,\n",
    "    \"dataset_name\": \"private_gib01\",\n",
    "    \"trained_for\": \"fine-tuning peak detection\",\n",
    "    \"all_samples\": True,\n",
    "    \"samples\": None,\n",
    "    \"gpu_id\": 1,\n",
    "    \"enable_tensorboard\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d482e35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 6.1 – Start TL training\n",
    "\n",
    "We call the TL-specific training routine on `tl_model`.\n",
    "\n",
    "During training:\n",
    "\n",
    "- Reused layers are kept frozen (as configured).\n",
    "- New layers adapt to the target dataset.\n",
    "- Training logs and checkpoints are saved under the TL experiment directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385508d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Start TL training\n",
    "print(\"\\n Starting TL training...\")\n",
    "tl_model.train_tl(**train_params)\n",
    "print(\" TL training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfec546",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary – Notebook 4\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Loaded a pretrained ECG **ProductionModel** from the NeuLib model hub.\n",
    "- Built a **TransferLearningModel** that reuses part of the GRU encoder.\n",
    "- Decided how many layers to reuse and how many new layers to add.\n",
    "- Injected pretrained weights and set a freezing/unfreezing strategy.\n",
    "- Fine-tuned the TLModel on a target-domain ECG dataset.\n",
    "\n",
    "This shows how NeuLib supports practical **transfer learning pipelines**\n",
    "for biosignal processing with minimal custom code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c9bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Future plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralLib.architectures.upload_to_hugging import upload_production_model\n",
    "\n",
    "# Example\n",
    "token='inser_your_token'\n",
    "description = \"GRU-based model for ECG peak detection\"\n",
    "upload_production_model(local_dir=r'/Users/groupies/Documents/NOVAProjects/NeuralLib-fork/hugging_prodmodels/ECGDenoiser',\n",
    "                        repo_name='Nianfei/ECGDenoiserNL',\n",
    "                        token = token,\n",
    "                        model_name='ECGDenoiserNL',\n",
    "                        description=description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
